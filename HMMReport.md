#HMM Report
----
##Introduction

The goal of this assignment was to implement a Hidden Markov Model (HMM) to determine the loaction of a robot in a maze based on sensor information. In the problem, a robot is placed at a random location in a maze and makes random moves (North, South, East, and West. The map of the maze is known but the moves are not. Each square of the maze also has a given color. The robot posesses a sensor that can read the color of a square with 88% accurately, or for any square there is a 0.88 chance that the color the sensor reads is the color of the square, and a 0.04 chance that the color the sensor reads will be each of the three other colors. For example if a square is blue, out of 100 sensor readings 88 will say blue, 4 will say green, 4 will say red, and 4 will say yellow. These probabilities define a sensor model for the HMM. I combined this with a transition model of how the robot moves throught the maze to create an HMM using filtering. All of this funtionality is in the `HMM` class.

##Filtering

Filtering uses a transition model of the state space and a sensor model to update probability distributions represting likely states. The HMM `compute_distribution` method uses filtering to generate a sequence of probability distributions based on a sequence of sensor readings. The model used has two components:

**The transition model:** this is the probability of moving to any square given that the robot is in a particular location. These probabilities are represented in a 16 by 16 matrix. This matrix is multiplied by a flattened version (1 by 16) of the matrix representing location probabilities in the maze at timestep $t$ to generate the probbabilities for timestep $t + 1$. This matrix was generated by looking at all possible moves out of a given square and determining the possibility of that particular move being taken, in simplified code:

```python
trans_matrix = np.zeros((16, 16))  # empty

# loop through all squares
for square in maze:
	moves_out = self.get_moves(square)  # get possible next squares
	total_moves = len(moves_out)  # total possible moves
	
	# loop through moves, updating next square probabilities
	for move in moves_out:
		trans_matrix[move, square] += 1/total_moves
		
return trans_matrix
		
```
This matrix remains constant for any given maze and many of the values will be 0 as there are no moves from some squares to others.

**The sensor model:** this matrix is the probability of being at a certain square given a sensor reading. If the reading is red, the chances of being at a particular red square is 0.88 divided by the number of red squares. The chances of being at a particular square of any other color is 0.04 divided by the number of squares of that color. Thus four matrices are created for any maze, one for each color.

For a given timstep, the transition matrix would be multiplied by the previous distribution and then the result multiplied by the sensor matrix to generate a new probability distribution. The first timestep is assumed to have a uniform distribution across possible squares. The output for a given timestep might look like this:

```
square color: g
sensor color: g

robot location:
#.#.
....
.#..
A###

distribution:
[[0.         0.01       0.         0.01333333]
 [0.04       0.01333333 0.01       0.44      ]
 [0.01333333 0.         0.01       0.01      ]
 [0.44       0.         0.         0.        ]]
 
```
The distribution represents the 4 by 4 maze such that each point visually corresponds with a point in the maze. Notice that the walls have a probability of 0 as the robot cannot enter those squares. The sensor color and actual color are results generated by `test_HMM` in `test_HMM.py`. This function calls `gen_random_colors` to generate a random color map for the maze, then progresses the robot along a path, simulating the sensor at each step.
 
##Smoothing
I also implemented a smoothing approach to the HMM using the forward-backward algorithm.[^1] In this apprach, the entire sequence of sensor readings is considered. The `compute_distrib_smoothing` method of the HMM class creates two messages, one starting with an equal distribution for time 0 and progressing through the sequence, the other starting with an equal distribution for the final timestep and progressing backward. These messages are then combined to generate a smoothed sequence. In simplified code:

```python
def forward_backward(sequence):
	# loop through forward and backward generating distributions
	forward_vector = forward(sequence)
	backward_vector = backward(sequence)
	
	backward_vector.reverse()  # flip it to match forward
	
	smoothed_vector = []  # stores final smoothed sequence
	
	# multiply each timestep by each other
	for timestep, vector in forward_vector:
		vector *= backward_vector(timestep)
		vector = normalize(vector)
		smoothed_vector.append(vector)
	
	return smoothed_vector
```
I found that smoothing more accurately predicted robot location, with the highest probability in the distribution corresponding to the robot's actual location more than with just filtering. Smoothing also has the advantage of not assuming uniform distribution at time 0, as the backward aspect predicts starting points unequally.

[^1]: From *Artificial Intelligence: A Modern Approach*, pg. 576.